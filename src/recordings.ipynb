{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Acoustic Environments Spring 2022\n",
    "## Project 9: Headphone Equalization\n",
    "**Author: Alpar Gür**\n",
    "\n",
    "**Supervisor: David Bau**\n",
    "\n",
    "**Professor: Christoph Pörschmann**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import pyfar as pf\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as mp\n",
    "from numpy.fft import rfft, irfft\n",
    "from time import sleep\n",
    "\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 Built-in Microphone, Core Audio (2 in, 0 out)\n",
       "  1 Built-in Output, Core Audio (0 in, 2 out)\n",
       "  2 rekordbox Aggregate Device, Core Audio (0 in, 2 out)\n",
       "* 3 Babyface Pro (73019724), Core Audio (14 in, 14 out)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which audio input is selected\n",
    "sd.default.device = 3\n",
    "sd.query_devices()\n",
    "# set audio input device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_numpy_array(recording_path: str):\n",
    "    data, samplerate = sf.read(recording_path, dtype='float32')\n",
    "    return data, samplerate\n",
    "    \n",
    "def record_sound(duration: int, samplerate: int, channel_count: int):\n",
    "    return sd.rec(int(duration * samplerate), samplerate=samplerate, channels=channel_count)\n",
    "\n",
    "def play_sound(recording: ndarray, samplerate: int):\n",
    "    sd.play(recording, samplerate)\n",
    "\n",
    "def play_and_record(recording, samplerate, channels):\n",
    "    return sd.playrec(recording, samplerate, channels, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sweep wav file into ndarray\n",
    "sweep, fs = convert_audio_to_numpy_array(\"../sounds/input/Sweep20_20k.wav\")\n",
    "# play audio\n",
    "sd.play(sweep, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96480, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_rec = play_and_record(sweep, fs, 2)\n",
    "sweep_rec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(recording):\n",
    "    # define length of FFT (zero padding): at least double length of input\n",
    "    input_length = np.size(recording)\n",
    "    n = np.ceil(np.log2(input_length)) + 1\n",
    "    N_fft = int(pow(2, n))\n",
    "    N_single_sided = N_fft/2+1\n",
    "\n",
    "    return input_length, N_fft, N_single_sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_t_to_f(audio, recording, N_fft):\n",
    "    y_l = recording[:, 0] # left channel\n",
    "    y_r = recording[:, 1] # right channel\n",
    "\n",
    "    print(np.shape(recording))\n",
    "\n",
    "    X_f = rfft(audio, N_fft)\n",
    "    Yl_f = rfft(y_l, N_fft)\n",
    "    Yr_f = rfft(y_r, N_fft)\n",
    "\n",
    "    return y_l, y_r, X_f, Yl_f, Yr_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_filter(X_f, Yl_f, Yr_f):\n",
    "    X_f_inv = 1 / X_f\n",
    "    Hl_f = Yl_f * X_f_inv\n",
    "    Hr_f = Yr_f * X_f_inv\n",
    "\n",
    "    return Hl_f, Hr_f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_f_to_t(Hl_f, Hr_f, N_fft, input_length):\n",
    "    # backward transform\n",
    "    hl = irfft(Hl_f, N_fft)\n",
    "    hr = irfft(Hr_f, N_fft)\n",
    "    # truncate to original length\n",
    "    hl = hl[:int(input_length/2)]\n",
    "    hr = hr[:int(input_length/2)]\n",
    "\n",
    "    return hl, hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sweep_stereo(y_l, y_r):\n",
    "    sweep_stereo = np.array([y_l, y_r]).transpose()\n",
    "    return sweep_stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_impulse_response(hl, hr):\n",
    "    impulse_response = np.array([hl, hr]).transpose()#.astype('float32')\n",
    "    return impulse_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sweep(recording, fs, headphone, epoch):\n",
    "    write(f\"../sounds/sweeps/{headphone}/{headphone}_sweep_{epoch}.wav\", fs, recording)\n",
    "\n",
    "def write_impulse_response(recording, fs, headphone, epoch):\n",
    "    write(f\"../sounds/impulse_responses/{headphone}/{headphone}_IR_{epoch}.wav\", fs, recording)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolve(audio, recording, fs):\n",
    "    input_length, N_fft, N_single_sided = pad_array(recording)\n",
    "    y_l, y_r, X_f, Yl_f, Yr_f = transform_t_to_f(audio, recording, N_fft)\n",
    "    Hl_f, Hr_f = compute_filter(X_f, Yl_f, Yr_f)\n",
    "    hl, hr = transform_f_to_t(Hl_f, Hr_f, N_fft, input_length)\n",
    "    sweep_stereo = make_sweep_stereo(y_l, y_r)\n",
    "    impulse_response = create_impulse_response(hl, hr)\n",
    "\n",
    "    return sweep_stereo, impulse_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Headphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96480, 2)\n",
      "(96480, 2)\n",
      "(96480, 2)\n"
     ]
    }
   ],
   "source": [
    "headphone = 'beyer_dynamics_dt_990_pro'\n",
    "fs = 48000\n",
    "audio = sweep\n",
    "epoch = 3\n",
    "\n",
    "for i in range(epoch):\n",
    "    recording = play_and_record(audio, fs, 2)\n",
    "    sweep_stereo, impulse_response = deconvolve(audio, recording, fs)\n",
    "    \n",
    "    maxAmp = np.max(np.max(sweep_stereo))\n",
    "    sweep_stereo /= maxAmp\n",
    "    write_sweep(sweep_stereo, fs, headphone, i)\n",
    "\n",
    "    write_impulse_response(impulse_response, fs, headphone, i)\n",
    "    \n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot imported signals \n",
    "# fig, (ax1, ax2, ax3) = mp.subplots(1, 3, figsize=(15, 2))\n",
    "# \n",
    "# ax1.set_title('Excitation Sweep')\n",
    "# ax1.set_ylabel(\"Frequency [Hz]\")\n",
    "# ax1.set_xlabel(\"Time [s]\")\n",
    "# ax2.set_title('Recorded Sweep Left Ear')\n",
    "# ax2.set_xlabel(\"Time [s]\")\n",
    "# ax3.set_title('Recorded Sweep Right Ear')\n",
    "# ax3.set_xlabel(\"Time [s]\")\n",
    "# \n",
    "# v_min = -120\n",
    "# v_max = -50\n",
    "# \n",
    "# _, _, _, cax1 = ax1.specgram(sweep+0.000001, NFFT=512, Fs=fs, noverlap=300, vmin=v_min, vmax=v_max)\n",
    "# _, _, _, cax2 = ax2.specgram(y_l, NFFT=512, Fs=fs, noverlap=300, vmin=v_min, vmax=v_max);\n",
    "# _, _, _, cax3 = ax3.specgram(y_r, NFFT=512, Fs=fs, noverlap=300, vmin=v_min, vmax=v_max);\n",
    "# \n",
    "# fig.colorbar(cax3, ax=ax3).set_label('Intensity [dB]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# f_vec = np.linspace(0, sweep_fs/2, int(N_single_sided))\n",
    "# fig, (ax1, ax2, ax3) = mp.subplots(1, 3, figsize=(15, 2), sharex=True)\n",
    "# ax1.set_title('Excitation')\n",
    "# ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "# ax1.set_ylabel(\"Magnitude [dB]\")\n",
    "# \n",
    "# ax2.set_title('Recorded Left')\n",
    "# ax2.set_xlabel(\"Frequency [Hz]\")\n",
    "# \n",
    "# ax3.set_title('Recorded Right')\n",
    "# ax3.set_xlabel(\"Frequency [Hz]\")\n",
    "# \n",
    "# ax1.semilogx(f_vec, 20*np.log10(np.abs(X_f)))\n",
    "# ax2.semilogx(f_vec, 20*np.log10(np.abs(Yl_f)))\n",
    "# ax3.semilogx(f_vec, 20*np.log10(np.abs(Yr_f)))\n",
    "# \n",
    "# ax1.set_xlim([20, sweep_fs/2]);\n",
    "#ax1.set_ylim([100, 150]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_f_inv = 1 / X_f\n",
    "#Hl_f = Yl_f * X_f_inv\n",
    "#Hr_f = Yr_f * X_f_inv\n",
    "#\n",
    "## plot\n",
    "#fig, (ax1, ax2) = mp.subplots(1, 2, figsize=(15, 2), sharex=True, sharey=True)\n",
    "#ax1.set_title('HRTF L')\n",
    "#ax1.set_xlabel(\"Frequency [Hz]\")\n",
    "#ax1.set_ylabel(\"Magnitude [dB]\")\n",
    "#\n",
    "#ax2.set_title('HRTF R')\n",
    "#ax2.set_xlabel(\"Frequency [Hz]\")\n",
    "#\n",
    "#ax1.semilogx(f_vec, 20*np.log10(np.abs(Hl_f)))\n",
    "#ax2.semilogx(f_vec, 20*np.log10(np.abs(Hr_f)))\n",
    "#\n",
    "#ax1.set_xlim([20, sweep_fs/2]);\n",
    "#ax1.set_ylim([-70, 20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward transform\n",
    "# hl = irfft(Hl_f, N_fft)\n",
    "# hr = irfft(Hr_f, N_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate to original length\n",
    "# hl = hl[:int(input_length/2)]\n",
    "# hr = hr[:int(input_length/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_vec = np.arange(0, np.size(hl)) / fs\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = mp.subplots(2, 2, figsize=(15, 8), sharex='row', sharey='row')\n",
    "# \n",
    "# ax1.plot(t_vec, 20*np.log10(np.abs(hl)))\n",
    "# ax1.set_title(\"IR Left Logarithmic\")\n",
    "# ax1.set_ylim([-120, 12])\n",
    "# ax1.set_ylabel(\"Magnitude [dB]\")\n",
    "# \n",
    "# ax2.plot(t_vec, 20*np.log10(np.abs(hr)))\n",
    "# ax2.set_title(\"IR Right Logarithmic\")\n",
    "# ax2.set_ylim([-120, 12])\n",
    "# \n",
    "# ax3.plot(t_vec, hl)\n",
    "# ax3.set_title(\"IR Left (zoomed in)\")\n",
    "# ax3.set_xlabel('Seconds');\n",
    "# ax3.set_ylabel(\"Amplitude\")\n",
    "# \n",
    "# ax4.plot(t_vec, hr)\n",
    "# ax4.set_title(\"IR Right (zoomed in)\")\n",
    "# ax4.set_xlabel('Seconds');\n",
    "# \n",
    "# \n",
    "# ax3.set_xlim([0.257, 0.27])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "427ec8dedc79bd1f7ae1a93cdbd521680c768e6af83d5a1520a37fa2234ce1d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
